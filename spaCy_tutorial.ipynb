{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy spacy-lookups-data\n",
    "#!pip install spacy-llm spacy-transformers\n",
    "#python -m spacy download en_core_web_trf #Accuracy\n",
    "#python -m spacy download en_core_web_sm #Efficiency\n",
    "#python -m spacy download en_core_web_lg #with vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "import en_core_web_trf\n",
    "nlp = en_core_web_trf.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central data structures in spaCy are the Language class, the Vocab and the Doc object. The Language class is used to process a text and turn it into a Doc object. It’s typically stored as a variable called nlp. The Doc object owns the sequence of tokens and all their annotations. By centralizing strings, word vectors and lexical attributes in the Vocab, we avoid storing multiple copies of this data. This saves memory, and ensures there’s a single source of truth.\n",
    "\n",
    "Text annotations are also designed to allow a single source of truth: the Doc object owns the data, and Span and Token are views that point into it. The Doc object is constructed by the Tokenizer, and then modified in place by the components of the pipeline. The Language object coordinates these components. It takes raw text and sends it through the pipeline, returning an annotated document. It also orchestrates training and serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'PRON'), ('is', 'AUX'), ('a', 'DET'), ('sentence', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This is a sentence.\")\n",
    "print([(w.text, w.pos_) for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call nlp on a text, spaCy first tokenizes the text to produce a Doc object. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. The pipeline used by the trained pipelines typically include a tagger, a lemmatizer, a parser and an entity recognizer. Each pipeline component returns the processed Doc, which is then passed on to the next component.\n",
    "\n",
    "The tokenizer is a “special” component and isn’t part of the regular pipeline. It also doesn’t show up in nlp.pipe_names. The reason is that there can only really be one tokenizer, and while all other pipeline components take a Doc and return it, the tokenizer takes a string of text and turns it into a Doc.\n",
    "\n",
    "The capabilities of a processing pipeline always depend on the components, their models and how they were trained. For example, a pipeline for named entity recognition needs to include a trained named entity recognizer component with a statistical model and weights that enable it to make predictions of entity labels. This is why each pipeline specifies its components and their settings in the config.\n",
    "\n",
    "Order of components: it matters if you add the EntityRuler before or after the statistical entity recognizer: if it’s added before, the entity recognizer will take the existing entities into account when making predictions. The EntityLinker, which resolves named entities to knowledge base IDs, should be preceded by a pipeline component that recognizes entities such as the EntityRecognizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER classes\n",
    "\n",
    "## EntityRecognizer\n",
    "\n",
    "A transition-based named entity recognition component. The entity recognizer identifies non-overlapping labelled spans of tokens. The transition-based algorithm used encodes certain assumptions that are effective for “traditional” named entity recognition tasks, but may not be a good fit for every span identification problem. Specifically, the loss function optimizes for whole entity accuracy, so if your inter-annotator agreement on boundary tokens is low, the component will likely perform poorly on your problem. The transition-based algorithm also assumes that the most decisive information about your entities will be close to their initial tokens. If your entities are long and characterized by tokens in their middle, the component will likely not be a good fit for your task.\n",
    "\n",
    "## EntityRuler\n",
    "\n",
    "The entity ruler lets you add spans to the Doc.ents using token-based rules or exact phrase matches. It can be combined with the statistical EntityRecognizer to boost accuracy, or used on its own to implement a purely rule-based entity recognition system. For usage examples, see the docs on [rule-based entity recognition](https://spacy.io/usage/rule-based-matching#entityruler).\n",
    "\n",
    "## EntityLinker\n",
    "\n",
    "An EntityLinker component disambiguates textual mentions (tagged as named entities) to unique identifiers, grounding the named entities into the “real world”. It requires a KnowledgeBase, as well as a function to generate plausible candidates from that KnowledgeBase given a certain textual mention, and a machine learning model to pick the right candidate, given the local context of the mention. EntityLinker defaults to using the InMemoryLookupKB implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the transformer models, ner listens to the transformer component, \n",
    "# so you can disable all components related tagging, parsing, and lemmatization.\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\", disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy with LLMs (rather than BERT-based models)\n",
    "\n",
    "The spacy-llm package integrates Large Language Models (LLMs) into spaCy pipelines, featuring a modular system for fast prototyping and prompting, and turning unstructured responses into robust outputs for various NLP tasks, no training data required.\n",
    "\n",
    "Supports OpenSource HuggingFace models and integrates with LangChain.\n",
    "\n",
    "Tasks available out of the box: Named Entity Recognition; Text classification; Lemmatization; Relationship extraction; Sentiment analysis; Span categorization; Summarization. Easy implementation of your own functions via spaCy's registry for custom prompting, parsing and model integrations.\n",
    "\n",
    "You can quickly initialize a pipeline with components powered by LLM prompts, and freely mix in components powered by other approaches. As your project progresses, you can look at replacing some or all of the LLM-powered components as you require.\n",
    "\n",
    "Of course, there can be components in your system for which the power of an LLM is fully justified. If you want a system that can synthesize information from multiple documents in subtle ways and generate a nuanced summary for you, bigger is better. However, even if your production system needs an LLM for some of the task, that doesn't mean you need an LLM for all of it. Maybe you want to use a cheap text classification model to help you find the texts to summarize, or maybe you want to add a rule-based system to sanity check the output of the summary. These before-and-after tasks are much easier with a mature and well-thought-out library, which is exactly what spaCy provides.\n",
    "\n",
    "The task and the model have to be supplied to the llm pipeline component using the config system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mlaprise/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/spacy_llm/models/rest/openai/model.py:25: UserWarning: Could not find the API key to access the OpenAI API. Ensure you have an API key set up via https://platform.openai.com/account/api-keys, then make it available as an environment variable 'OPENAI_API_KEY'.\n",
      "  warnings.warn(\n",
      "/Users/mlaprise/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/spacy_llm/models/rest/openai/model.py:61: UserWarning: Authentication with provided API key failed. Please double-check you provided the correct credentials.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Example creating the component directly\u001b[39;00m\n\u001b[1;32m      2\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mblank(\u001b[39m\"\u001b[39m\u001b[39men\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m llm_ner \u001b[39m=\u001b[39m nlp\u001b[39m.\u001b[39;49madd_pipe(\u001b[39m\"\u001b[39;49m\u001b[39mllm_ner\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m llm_ner\u001b[39m.\u001b[39madd_label(\u001b[39m\"\u001b[39m\u001b[39mPERSON\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m llm_ner\u001b[39m.\u001b[39madd_label(\u001b[39m\"\u001b[39m\u001b[39mLOCATION\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/spacy/language.py:814\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    810\u001b[0m     pipe_component, factory_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_pipe_from_source(\n\u001b[1;32m    811\u001b[0m         factory_name, source, name\u001b[39m=\u001b[39mname\n\u001b[1;32m    812\u001b[0m     )\n\u001b[1;32m    813\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 814\u001b[0m     pipe_component \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_pipe(\n\u001b[1;32m    815\u001b[0m         factory_name,\n\u001b[1;32m    816\u001b[0m         name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    817\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    818\u001b[0m         raw_config\u001b[39m=\u001b[39;49mraw_config,\n\u001b[1;32m    819\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m    820\u001b[0m     )\n\u001b[1;32m    821\u001b[0m pipe_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_pipe_index(before, after, first, last)\n\u001b[1;32m    822\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipe_meta[name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_factory_meta(factory_name)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/spacy/language.py:702\u001b[0m, in \u001b[0;36mLanguage.create_pipe\u001b[0;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    699\u001b[0m cfg \u001b[39m=\u001b[39m {factory_name: config}\n\u001b[1;32m    700\u001b[0m \u001b[39m# We're calling the internal _fill here to avoid constructing the\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[39m# registered functions twice\u001b[39;00m\n\u001b[0;32m--> 702\u001b[0m resolved \u001b[39m=\u001b[39m registry\u001b[39m.\u001b[39;49mresolve(cfg, validate\u001b[39m=\u001b[39;49mvalidate)\n\u001b[1;32m    703\u001b[0m filled \u001b[39m=\u001b[39m registry\u001b[39m.\u001b[39mfill({\u001b[39m\"\u001b[39m\u001b[39mcfg\u001b[39m\u001b[39m\"\u001b[39m: cfg[factory_name]}, validate\u001b[39m=\u001b[39mvalidate)[\u001b[39m\"\u001b[39m\u001b[39mcfg\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    704\u001b[0m filled \u001b[39m=\u001b[39m Config(filled)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/confection/__init__.py:756\u001b[0m, in \u001b[0;36mregistry.resolve\u001b[0;34m(cls, config, schema, overrides, validate)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    748\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresolve\u001b[39m(\n\u001b[1;32m    749\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    754\u001b[0m     validate: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    755\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m--> 756\u001b[0m     resolved, _ \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_make(\n\u001b[1;32m    757\u001b[0m         config, schema\u001b[39m=\u001b[39;49mschema, overrides\u001b[39m=\u001b[39;49moverrides, validate\u001b[39m=\u001b[39;49mvalidate, resolve\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    758\u001b[0m     )\n\u001b[1;32m    759\u001b[0m     \u001b[39mreturn\u001b[39;00m resolved\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/confection/__init__.py:805\u001b[0m, in \u001b[0;36mregistry._make\u001b[0;34m(cls, config, schema, overrides, resolve, validate)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_interpolated:\n\u001b[1;32m    804\u001b[0m     config \u001b[39m=\u001b[39m Config(orig_config)\u001b[39m.\u001b[39minterpolate()\n\u001b[0;32m--> 805\u001b[0m filled, _, resolved \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_fill(\n\u001b[1;32m    806\u001b[0m     config, schema, validate\u001b[39m=\u001b[39;49mvalidate, overrides\u001b[39m=\u001b[39;49moverrides, resolve\u001b[39m=\u001b[39;49mresolve\n\u001b[1;32m    807\u001b[0m )\n\u001b[1;32m    808\u001b[0m filled \u001b[39m=\u001b[39m Config(filled, section_order\u001b[39m=\u001b[39msection_order)\n\u001b[1;32m    809\u001b[0m \u001b[39m# Check that overrides didn't include invalid properties not in config\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/confection/__init__.py:860\u001b[0m, in \u001b[0;36mregistry._fill\u001b[0;34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[0m\n\u001b[1;32m    858\u001b[0m     schema\u001b[39m.\u001b[39m__fields__[key] \u001b[39m=\u001b[39m copy_model_field(field, Any)\n\u001b[1;32m    859\u001b[0m promise_schema \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmake_promise_schema(value, resolve\u001b[39m=\u001b[39mresolve)\n\u001b[0;32m--> 860\u001b[0m filled[key], validation[v_key], final[key] \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_fill(\n\u001b[1;32m    861\u001b[0m     value,\n\u001b[1;32m    862\u001b[0m     promise_schema,\n\u001b[1;32m    863\u001b[0m     validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m    864\u001b[0m     resolve\u001b[39m=\u001b[39;49mresolve,\n\u001b[1;32m    865\u001b[0m     parent\u001b[39m=\u001b[39;49mkey_parent,\n\u001b[1;32m    866\u001b[0m     overrides\u001b[39m=\u001b[39;49moverrides,\n\u001b[1;32m    867\u001b[0m )\n\u001b[1;32m    868\u001b[0m reg_name, func_name \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_constructor(final[key])\n\u001b[1;32m    869\u001b[0m args, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mparse_args(final[key])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/confection/__init__.py:877\u001b[0m, in \u001b[0;36mregistry._fill\u001b[0;34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[0m\n\u001b[1;32m    874\u001b[0m     getter \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget(reg_name, func_name)\n\u001b[1;32m    875\u001b[0m     \u001b[39m# We don't want to try/except this and raise our own error\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     \u001b[39m# here, because we want the traceback if the function fails.\u001b[39;00m\n\u001b[0;32m--> 877\u001b[0m     getter_result \u001b[39m=\u001b[39m getter(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    878\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    879\u001b[0m     \u001b[39m# We're not resolving and calling the function, so replace\u001b[39;00m\n\u001b[1;32m    880\u001b[0m     \u001b[39m# the getter_result with a Promise class\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     getter_result \u001b[39m=\u001b[39m Promise(\n\u001b[1;32m    882\u001b[0m         registry\u001b[39m=\u001b[39mreg_name, name\u001b[39m=\u001b[39mfunc_name, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    883\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/spacy_llm/models/rest/openai/registry.py:106\u001b[0m, in \u001b[0;36mopenai_gpt_3_5_v2\u001b[0;34m(config, name, strict, max_tries, interval, max_request_time)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39m@registry\u001b[39m\u001b[39m.\u001b[39mllm_models(\u001b[39m\"\u001b[39m\u001b[39mspacy.GPT-3-5.v2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopenai_gpt_3_5_v2\u001b[39m(\n\u001b[1;32m     86\u001b[0m     config: Dict[Any, Any] \u001b[39m=\u001b[39m SimpleFrozenDict(temperature\u001b[39m=\u001b[39m_DEFAULT_TEMPERATURE),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m     max_request_time: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m OpenAI\u001b[39m.\u001b[39mDEFAULT_MAX_REQUEST_TIME,\n\u001b[1;32m     97\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Callable[[Iterable[\u001b[39mstr\u001b[39m]], Iterable[\u001b[39mstr\u001b[39m]]:\n\u001b[1;32m     98\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns OpenAI instance for 'gpt-3.5' model using REST to prompt API.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[1;32m    100\u001b[0m \u001b[39m    config (Dict[Any, Any]): LLM config passed on to the model's initialization.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m    DOCS: https://spacy.io/api/large-language-models#models\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mreturn\u001b[39;00m OpenAI(\n\u001b[1;32m    107\u001b[0m         name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    108\u001b[0m         endpoint\u001b[39m=\u001b[39;49mEndpoints\u001b[39m.\u001b[39;49mCHAT\u001b[39m.\u001b[39;49mvalue,\n\u001b[1;32m    109\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    110\u001b[0m         strict\u001b[39m=\u001b[39;49mstrict,\n\u001b[1;32m    111\u001b[0m         max_tries\u001b[39m=\u001b[39;49mmax_tries,\n\u001b[1;32m    112\u001b[0m         interval\u001b[39m=\u001b[39;49minterval,\n\u001b[1;32m    113\u001b[0m         max_request_time\u001b[39m=\u001b[39;49mmax_request_time,\n\u001b[1;32m    114\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/spacy_llm/models/rest/base.py:65\u001b[0m, in \u001b[0;36mREST.__init__\u001b[0;34m(self, name, endpoint, config, strict, max_tries, interval, max_request_time)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_request_time \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_model()\n\u001b[0;32m---> 65\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_verify_auth()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/spacy_llm/models/rest/openai/model.py:70\u001b[0m, in \u001b[0;36mOpenAI._verify_auth\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m     67\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError accessing api.openai.com (\u001b[39m\u001b[39m{\u001b[39;00mr\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m): \u001b[39m\u001b[39m{\u001b[39;00mr\u001b[39m.\u001b[39mtext\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m         )\n\u001b[0;32m---> 70\u001b[0m response \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39;49mjson()[\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     71\u001b[0m models \u001b[39m=\u001b[39m [response[i][\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(response))]\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m models:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "# Example creating the component directly\n",
    "nlp = spacy.blank(\"en\")\n",
    "llm_ner = nlp.add_pipe(\"llm_ner\")\n",
    "llm_ner.add_label(\"PERSON\")\n",
    "llm_ner.add_label(\"LOCATION\")\n",
    "nlp.initialize()\n",
    "doc = nlp(\"Jack and Jill rode up the hill in Les Deux Alpes\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mlaprise/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/spacy_llm/models/hf/base.py:99: UserWarning: Couldn't find a CUDA GPU, so the setting 'device_map:auto' will be used, which may result in the LLM being loaded (partly) on the CPU or even the hard disk, which may be slow. Install cuda to be able to load and run the LLM on the GPU instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93bc1ff940274688b460456ac0a8d4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/819 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155fd5f323324f828502ac9a6f3c8bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)instruct_pipeline.py:   0%|          | 0.00/9.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/databricks/dolly-v2-3b:\n",
      "- instruct_pipeline.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139986613eb4449da0f9408ee705425a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/5.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e200d7b315bd4c069d99409e697bfd5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/450 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cdbbaab9454d40b831f77c2eaf4d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb729afd8644708a4a438819daf2e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mlaprise/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/transformers/generation/utils.py:723: UserWarning: MPS: no support for int64 repeats mask, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Repeat.mm:236.)\n",
      "  input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS does not support cumsum op with int64 input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy_llm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m assemble\n\u001b[1;32m      4\u001b[0m nlp \u001b[39m=\u001b[39m assemble(\u001b[39m\"\u001b[39m\u001b[39mconfig.cfg\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m doc \u001b[39m=\u001b[39m nlp(\u001b[39m\"\u001b[39;49m\u001b[39mJack and Jill rode up the hill in Les Deux Alpes\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m([(ent\u001b[39m.\u001b[39mtext, ent\u001b[39m.\u001b[39mlabel_) \u001b[39mfor\u001b[39;00m ent \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39ments])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/spacy/language.py:1047\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1047\u001b[0m     error_handler(name, proc, [doc], e)\n\u001b[1;32m   1048\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(doc, Doc):\n\u001b[1;32m   1049\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE005\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, returned_type\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m(doc)))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/spacy/util.py:1724\u001b[0m, in \u001b[0;36mraise_error\u001b[0;34m(proc_name, proc, docs, e)\u001b[0m\n\u001b[1;32m   1723\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_error\u001b[39m(proc_name, proc, docs, e):\n\u001b[0;32m-> 1724\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/spacy/language.py:1042\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m   1041\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1042\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1044\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/spacy_llm/pipeline/llm.py:156\u001b[0m, in \u001b[0;36mLLMWrapper.__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, doc: Doc) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Doc:\n\u001b[1;32m    151\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply the LLM wrapper to a Doc instance.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \n\u001b[1;32m    153\u001b[0m \u001b[39m    doc (Doc): The Doc instance to process.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39m    RETURNS (Doc): The processed Doc.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_docs([doc])\n\u001b[1;32m    157\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(docs[\u001b[39m0\u001b[39m], Doc)\n\u001b[1;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m docs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/spacy_llm/pipeline/llm.py:210\u001b[0m, in \u001b[0;36mLLMWrapper._process_docs\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    206\u001b[0m n_iters \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_io \u001b[39melse\u001b[39;00m \u001b[39m2\u001b[39m\n\u001b[1;32m    207\u001b[0m prompts_iters \u001b[39m=\u001b[39m tee(\n\u001b[1;32m    208\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task\u001b[39m.\u001b[39mgenerate_prompts(noncached_doc_batch), n_iters\n\u001b[1;32m    209\u001b[0m )\n\u001b[0;32m--> 210\u001b[0m responses_iters \u001b[39m=\u001b[39m tee(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(prompts_iters[\u001b[39m0\u001b[39;49m]), n_iters)\n\u001b[1;32m    211\u001b[0m \u001b[39mfor\u001b[39;00m prompt, response, doc \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[1;32m    212\u001b[0m     prompts_iters[\u001b[39m1\u001b[39m], responses_iters[\u001b[39m1\u001b[39m], noncached_doc_batch\n\u001b[1;32m    213\u001b[0m ):\n\u001b[1;32m    214\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mGenerated prompt for doc: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, doc\u001b[39m.\u001b[39mtext, prompt)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/spacy_llm/models/hf/dolly.py:25\u001b[0m, in \u001b[0;36mDolly.__call__\u001b[0;34m(self, prompts)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, prompts: Iterable[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterable[\u001b[39mstr\u001b[39m]:  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Queries Dolly HF model.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m    pipeline (transformers.pipeline): Transformers pipeline to query.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m    prompts (Iterable[str]): Prompts to query Dolly model with.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m    RETURNS (Iterable[str]): Prompt responses.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m     26\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(pr, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_config_run)[\u001b[39m0\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mgenerated_text\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m pr \u001b[39min\u001b[39;49;00m prompts\n\u001b[1;32m     27\u001b[0m     ]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/spacy_llm/models/hf/dolly.py:26\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, prompts: Iterable[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterable[\u001b[39mstr\u001b[39m]:  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Queries Dolly HF model.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m    pipeline (transformers.pipeline): Transformers pipeline to query.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m    prompts (Iterable[str]): Prompts to query Dolly model with.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m    RETURNS (Iterable[str]): Prompt responses.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m---> 26\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(pr, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_config_run)[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m pr \u001b[39min\u001b[39;00m prompts\n\u001b[1;32m     27\u001b[0m     ]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/transformers/pipelines/base.py:1120\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1113\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1114\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         )\n\u001b[1;32m   1118\u001b[0m     )\n\u001b[1;32m   1119\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/transformers/pipelines/base.py:1127\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1126\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1127\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1128\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1129\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/transformers/pipelines/base.py:1026\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1025\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1026\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1027\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1028\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/databricks/dolly-v2-3b/f6c9be08f16fe4d3a719bee0a4a7c7415b5c65df/instruct_pipeline.py:132\u001b[0m, in \u001b[0;36mInstructionTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     in_b \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 132\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    133\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mdevice),\n\u001b[1;32m    134\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mdevice) \u001b[39mif\u001b[39;49;00m attention_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    135\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs,\n\u001b[1;32m    137\u001b[0m )\n\u001b[1;32m    139\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/transformers/generation/utils.py:1572\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1564\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1565\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1566\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1567\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1568\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1569\u001b[0m     )\n\u001b[1;32m   1571\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1572\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1573\u001b[0m         input_ids,\n\u001b[1;32m   1574\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1575\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1576\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1577\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1578\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1579\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1580\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1581\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1582\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1583\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1584\u001b[0m     )\n\u001b[1;32m   1586\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1587\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/transformers/generation/utils.py:2616\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2613\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   2615\u001b[0m \u001b[39m# prepare model inputs\u001b[39;00m\n\u001b[0;32m-> 2616\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2618\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m   2619\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[1;32m   2620\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[1;32m   2621\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   2622\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   2623\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2624\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/lamaglama/lib/python3.11/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:723\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.prepare_inputs_for_generation\u001b[0;34m(self, input_ids, past_key_values, attention_mask, inputs_embeds, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m position_ids \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mposition_ids\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    721\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m position_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     \u001b[39m# create position_ids on the fly for batch generation\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     position_ids \u001b[39m=\u001b[39m attention_mask\u001b[39m.\u001b[39;49mlong()\u001b[39m.\u001b[39;49mcumsum(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    724\u001b[0m     position_ids\u001b[39m.\u001b[39mmasked_fill_(attention_mask \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    725\u001b[0m     \u001b[39mif\u001b[39;00m past_key_values:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS does not support cumsum op with int64 input"
     ]
    }
   ],
   "source": [
    "# Example using a HugingFace model\n",
    "from spacy_llm.util import assemble\n",
    "\n",
    "nlp = assemble(\"config.cfg\")\n",
    "doc = nlp(\"Jack and Jill rode up the hill in Les Deux Alpes\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: None of the tutorials example run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Choice of model for spacy-llm__\n",
    "\n",
    "All built-in models are registered in llm_models. If no model is specified, the repo currently connects to the OpenAI API by default using REST, and accesses the \"gpt-3.5-turbo\" model.\n",
    "\n",
    "Currently three different approaches to use LLMs are supported:\n",
    "\n",
    "1. spacy-llms native REST interface. This is the default for all hosted models (e. g. OpenAI, Cohere, Anthropic, …).\n",
    "\n",
    "2. A HuggingFace integration that allows to run a limited set of HF models locally.\n",
    "\n",
    "3. A LangChain integration that allows to run any model supported by LangChain (hosted or locally).\n",
    "\n",
    "Approaches 1. and 2 are the default for hosted model and local models, respectively. Alternatively you can use LangChain to access hosted or local models by specifying one of the models registered with the langchain. prefix.\n",
    "\n",
    "Includes: `spacy.Llama2.v1`:\tLlama2 models through HuggingFace; `spacy.OpenLLaMA.v1`:\tOpenLLaMA models through HuggingFace. Note that the chat models variants of Llama 2 are currently not supported. This is because they need a particular prompting setup and don’t add any discernible benefits in the use case of spacy-llm (i. e. no interactive chat) compared to the completion model variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamaglama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
